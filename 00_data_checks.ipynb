{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>DATA CHECKS.IPYNB</b><h3>\n",
    "<p>Loads in one file from jatos folder</p>\n",
    "<p>Allows you to quickly look at the data</p>\n",
    "<p>For the sole purpose of giving prolific bonuses/approvals in real time</p>\n",
    "<p>Will also allow adjustments to recruitment to be made in real time </p>\n",
    "<p>Will make exclusion decisions later</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import jsonlines\n",
    "from functools import reduce\n",
    "import statistics\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1: LOAD IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this is dummy data --> created in a test runthrough of the task by the experimenter\n",
    "    ##have removed all refs to prolific ID and demographic info\n",
    "file_name=r\"U:\\Documents\\Disgust learning project\\pilot\\creating_analysis_pipeline -wip\\jatos_files\\jatos_results_20240221150726.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD IN DATA\n",
    "i = 0\n",
    "with jsonlines.open(file_name) as reader: ##second pilot data\n",
    "    for line in reader:\n",
    "        if i ==0:\n",
    "            df = pd.DataFrame(line)\n",
    "            i +=1\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame(line)])\n",
    "            i +=1\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: VIDEO RATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_vids_df=df[df.trial_var==\"rate_stim\"]\n",
    "rating_vids_df['response']=rating_vids_df['response'].replace('  ', np.nan)\n",
    "rating_vids_df= rating_vids_df.dropna(subset=['response'])\n",
    "rating_vids_df.sort_values(by=[\"stimulus\", \"trial_index\"], inplace=True) ##groups dataframe by video type - allows you to extract 1st, 2nd and 3rd presentation of each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with all ratings (for one participant)\n",
    "rating_vids_a=[]\n",
    "rating_vids_b=[]\n",
    "rating_vids_c=[]\n",
    "\n",
    "vals = range(len(rating_vids_df.index))\n",
    "\n",
    "for val in vals:\n",
    "    stim = str(rating_vids_df.iloc[val].stimulus)\n",
    "    trial_type=rating_vids_df.iloc[val].type\n",
    "    response = rating_vids_df.iloc[val].response\n",
    "    unpleasant=response['Q0']\n",
    "    arousing=response['Q1']\n",
    "    disgusting=response['Q2']\n",
    "    frightening=response['Q3']\n",
    "\n",
    "    if \"0888.gif\" in stim:\n",
    "        vid=\"0888\"\n",
    "    elif \"1414.gif\" in stim:\n",
    "        vid=\"1414\"\n",
    "    elif \"1765.gif\" in stim:\n",
    "        vid=\"1765\"\n",
    "    elif \"1987.gif\" in stim:\n",
    "        vid=\"1987\"\n",
    "    elif \"2106.gif\" in stim:\n",
    "        vid=\"2106\"\n",
    "    elif \"0046.gif\" in stim:\n",
    "        vid = \"0046\"\n",
    "    elif \"0374.gif\" in stim:\n",
    "        vid = \"0374\"\n",
    "    elif \"0548.gif\" in stim:\n",
    "        vid = \"0548\"\n",
    "    elif \"0877.gif\" in stim:\n",
    "        vid = \"0877\"\n",
    "    elif \"1202.gif\" in stim:\n",
    "        vid = \"1202\"\n",
    "    else:\n",
    "        vid = \"ERROR\"\n",
    "\n",
    "    if val in range(0, 30, 3):\n",
    "        rating_vids_a.append({\n",
    "            'Vid' : vid,\n",
    "            'trial_type': trial_type,\n",
    "            'unpleasant_1': unpleasant,\n",
    "            'arousing_1': arousing,\n",
    "            'disgusting_1': disgusting,\n",
    "            'frightening_1': frightening,\n",
    "            'disgust_stim': 0,\n",
    "            'fear_stim': 0,\n",
    "        })\n",
    "    elif val in range(1,30,3):\n",
    "        rating_vids_b.append({\n",
    "            'Vid' : vid,\n",
    "            'trial_type': trial_type,\n",
    "            'unpleasant_2': unpleasant,\n",
    "            'arousing_2': arousing,\n",
    "            'disgusting_2': disgusting,\n",
    "            'frightening_2': frightening,\n",
    "            'disgust_stim': 0,\n",
    "            'fear_stim': 0,\n",
    "        })\n",
    "    elif val in range(2,30,3):\n",
    "        rating_vids_c.append({\n",
    "            'Vid' : vid,\n",
    "            'trial_type': trial_type,\n",
    "            'unpleasant_3': unpleasant,\n",
    "            'arousing_3': arousing,\n",
    "            'disgusting_3': disgusting,\n",
    "            'frightening_3': frightening,\n",
    "            'disgust_stim': 0,\n",
    "            'fear_stim': 0,\n",
    "        })\n",
    "\n",
    "rating_vids_a=pd.DataFrame(rating_vids_a)\n",
    "rating_vids_b=pd.DataFrame(rating_vids_b)\n",
    "rating_vids_c=pd.DataFrame(rating_vids_c)\n",
    "rating_vids=rating_vids_a.merge(rating_vids_b, on=['Vid', 'trial_type', 'disgust_stim', 'fear_stim'])\n",
    "rating_vids=rating_vids.merge(rating_vids_c, on=['Vid', 'trial_type', 'disgust_stim', 'fear_stim'])\n",
    "rating_vids=rating_vids[['Vid', 'trial_type','unpleasant_1', 'unpleasant_2', 'unpleasant_3', 'arousing_1', 'arousing_2', 'arousing_3', 'disgusting_1', 'disgusting_2', 'disgusting_3', 'frightening_1', 'frightening_2', 'frightening_3', 'disgust_stim', 'fear_stim']]\n",
    "\n",
    "#add which video was chosen for disgust and fear stim\n",
    "fear_stim=str(df.fear_stimulus.dropna())\n",
    "disgust_stim=str(df.disgust_stimulus.dropna())\n",
    "\n",
    "if \"0888.gif\" in disgust_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"0888\", ['disgust_stim']]=1\n",
    "elif \"1414.gif\" in disgust_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"1414\", ['disgust_stim']]=1\n",
    "elif \"1765.gif\" in disgust_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"1765\", ['disgust_stim']]=1\n",
    "elif \"1987.gif\" in disgust_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"1987\", ['disgust_stim']]=1\n",
    "elif \"2106.gif\" in disgust_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"2106\", ['disgust_stim']]=1\n",
    "else:\n",
    "       print(\"error\")\n",
    "\n",
    "if \"0046.gif\" in fear_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"0046\", ['fear_stim']]=1\n",
    "elif \"0374.gif\" in fear_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"0374\", ['fear_stim']]=1\n",
    "elif \"0548.gif\" in fear_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"0548\", ['fear_stim']]=1\n",
    "elif \"0877.gif\" in fear_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"0877\", ['fear_stim']]=1\n",
    "elif \"1202.gif\" in fear_stim:\n",
    "       rating_vids.loc[rating_vids['Vid']==\"1202\", ['fear_stim']]=1\n",
    "else:\n",
    "       print(\"error\")\n",
    "\n",
    "#add prolific ID\n",
    "#rating_vids['prolific_id']=df[0:1].prolific_id[0]\n",
    "rating_vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking chosen stim\n",
    "    ##want valence and arousal to be roughly similar\n",
    "    ##and not rated the fear or disugst images as 0 for fear/disgust (whichever the emotion of interest is)\n",
    "    \n",
    "chosen_stim=pd.concat([rating_vids[rating_vids.disgust_stim==1], rating_vids[rating_vids.fear_stim==1]])\n",
    "fig, ax = plt.subplots(nrows=2,ncols=2, sharey=False)\n",
    "fig.tight_layout(pad=2)\n",
    "\n",
    "ax[0,0].bar(['Disgust', 'Fear'], [np.mean(chosen_stim[chosen_stim.trial_type==\"disgust\"].unpleasant_2), np.mean(chosen_stim[chosen_stim.trial_type==\"fear\"].unpleasant_2)])\n",
    "ax[0,0].set_title(\"Valence ratings\")\n",
    "\n",
    "ax[0,1].bar(['Disgust', 'Fear'], [np.mean(chosen_stim[chosen_stim.trial_type==\"disgust\"].arousing_2), np.mean(chosen_stim[chosen_stim.trial_type==\"fear\"].arousing_2)])\n",
    "ax[0,1].set_title(\"Arousal ratings\")\n",
    "\n",
    "\n",
    "ax[1,0].bar(['Disgust', 'Fear'], [np.mean(chosen_stim[chosen_stim.trial_type==\"disgust\"].disgusting_2), np.mean(chosen_stim[chosen_stim.trial_type==\"fear\"].disgusting_2)])\n",
    "ax[1,0].set_title(\"Disgust ratings\")\n",
    "\n",
    "ax[1,1].bar(['Disgust', 'Fear'], [np.mean(chosen_stim[chosen_stim.trial_type==\"disgust\"].frightening_2), np.mean(chosen_stim[chosen_stim.trial_type==\"fear\"].frightening_2)])\n",
    "ax[1,1].set_title(\"Fear ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: BEHAVIOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructions quiz and practice % correct\n",
    "task_understood=pd.DataFrame([{'instruction_repeats': np.shape(df[df.trial_type==\"instructions\"])[0]}])\n",
    "\n",
    "practice_df=df[df.task==\"practice_task\"]\n",
    "practice_df=practice_df[practice_df['correct'].notna()]\n",
    "task_understood['practice_percent']=np.shape(practice_df[practice_df.correct==True])[0] / np.shape(practice_df)[0]\n",
    "\n",
    "#task_understood['prolific_id']=df[0:1].prolific_id[0]\n",
    "task_understood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot practice decisions\n",
    "plt.plot(range(0,10), practice_df.correct, 'o')\n",
    "plt.plot(np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention checks \n",
    "\n",
    "attention=df[df.trial_var==\"attention_check\"]\n",
    "index=df.index[df.trial_var==\"attention_check\"]\n",
    "if attention.loc[index[0]].response == {'Q0': ['Apple', 'Banana']}:\n",
    "    block1=2\n",
    "elif (attention.loc[0].response == \"{'Q0': ['Banana']}\") or (attention.loc[0].response == \"{'Q0': ['Spoon']}\"):\n",
    "            block1=1\n",
    "else:\n",
    "    block1=0\n",
    "\n",
    "if attention.loc[index[1]].response== {'Q0': ['Bowl', 'Spoon']}:\n",
    "    block2=2\n",
    "elif (attention.loc[1].response== \"{'Q0': ['Bowl']}\") or (attention.loc[1].response== \"{'Q0': ['Spoon']}\"):\n",
    "            block2=1\n",
    "else:\n",
    "    block2=0\n",
    "\n",
    "if attention.loc[index[2]].response== {'Q0': ['River', 'Mountain']}:\n",
    "    block3=2\n",
    "elif (attention.loc[2].response== \"{'Q0': ['River']}\") or (attention.loc[2].response== \"{'Q0': ['Mountain']}\"):\n",
    "    block3=1\n",
    "else:\n",
    "    block3=0\n",
    "\n",
    "attention_checks = pd.DataFrame({\n",
    "    'block': ['block 1', 'block 2', 'block 3'],\n",
    "    'correct' :[block1, block2, block3]\n",
    "})\n",
    "\n",
    "task_understood['attention_checks']=np.sum(attention_checks.correct)\n",
    "display(attention_checks)\n",
    "print(np.sum(attention_checks.correct)) ##must be over 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with just task data\n",
    "task=df[df.task==\"main_task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that contingencies were correct (should e about .8) -- sanity check that the task is working\n",
    "task.feedback_congruent.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create a dataframe for each block\n",
    "def create_df(block_name):\n",
    "    block_df=pd.DataFrame(columns=['n_trial', 'rt', 'stim_selected', 'correct_stim', 'correct', 'feedback', 'feedback_congruent', 'correct_count', 'trial_till_correct', 'reversal', 'block_no', 'prolific_id', 'timed_out'])\n",
    "    block=task[task.block_type==block_name]\n",
    "    block.reset_index(inplace=True)\n",
    "    block.drop(['level_0', 'index'], axis=1, inplace=True)\n",
    "\n",
    "    for i in set(block.n_trial):\n",
    "        trial=block[block.n_trial==i]\n",
    "        trial.reset_index(inplace=True)\n",
    "\n",
    "        row = []\n",
    "        if \"red\" in trial.feedback[2]:\n",
    "            feedback='incorrect'\n",
    "        else:\n",
    "            feedback='correct'\n",
    "\n",
    "        row.append({\n",
    "            'n_trial': trial.n_trial[0], #\n",
    "            'stim_selected': trial.stim_selected[0],#\n",
    "            'correct_stim': trial.correct_stim[0],#\n",
    "            'correct': trial.correct[0],#\n",
    "            'feedback': feedback,\n",
    "            'feedback_congruent': trial.feedback_congruent[2],\n",
    "            'correct_count': trial.correct_count[0],#\n",
    "            'trial_till_correct': trial.trial_till_correct[0],#\n",
    "            'rt': trial.rt[0],\n",
    "            'reversal': trial.reversal[0],#\n",
    "            'block_no': trial.block_no[0],#\n",
    "            #'prolific_id': trial.prolific_id[0],#\n",
    "            'timed_out': 0,\n",
    "            'time_taken': (block.time_elapsed.iloc[-1]-block.time_elapsed[0])/60000 ##in minutes\n",
    "        })\n",
    "        block_df=pd.concat([block_df, pd.DataFrame(row)])\n",
    "    block_df.reset_index(inplace=True)\n",
    "\n",
    "    #replace stimuli with 0 and 1 (for plotting)\n",
    "    stim=list(set(block_df.correct_stim.to_list()))\n",
    "    stim0=\"<img src='\"+str(stim[0])+\"'</img>\"\n",
    "    stim0b=\"  <img src='\"+str(stim[0])+\"'</img>\"\n",
    "    stim1=\"<img src='\"+str(stim[1])+\"'</img>\"\n",
    "    stim1b=\"  <img src='\"+str(stim[1])+\"'</img>\"\n",
    "\n",
    "    block_df.replace([stim[0], stim[1]], [0, 1], inplace=True)\n",
    "    block_df.replace([stim0, stim1], [0,1], inplace=True)\n",
    "    block_df.replace([stim0b, stim1b], [0,1], inplace=True)\n",
    "\n",
    "    #did they time out before reaching 7 reversals\n",
    "    short_block=block[block.trial_till_correct.notna()] ##removes trials after they timed out (if they did)\n",
    "    if short_block.iloc[-1].reversal==7.0 and short_block.iloc[-1].correct_count>=5:\n",
    "        block_df.timed_out=0\n",
    "    else:\n",
    "        block_df.timed_out=1\n",
    "    \n",
    "    ##did reach reversal criteria for inclusion\n",
    "    criteria=5\n",
    "    if short_block.iloc[-1].reversal>=criteria: ##if they reached more reversals than criteria\n",
    "        block_df['criteria']=0 ##pass\n",
    "    else:\n",
    "        block_df['criteria']=1 ##fail\n",
    "\n",
    "    return block_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgust_df=create_df(\"Disgust\")\n",
    "#disgust_df=create_block_df(\"Disgust\")\n",
    "plt.plot(disgust_df.stim_selected, 'o')\n",
    "plt.plot(disgust_df.correct_stim)\n",
    "\n",
    "if disgust_df.iloc[-1].reversal==7.0 and disgust_df.iloc[-1].correct_count==5:\n",
    "    task_understood['timed_out_d']=0\n",
    "    print(\"Passed!\")\n",
    "else:\n",
    "    task_understood['timed_out_d']=1\n",
    "    print(\"Timed out\")\n",
    "\n",
    "if disgust_df.iloc[0].criteria ==0:\n",
    "    task_understood['criteria_d']=0\n",
    "    print(\"Met criteria\")\n",
    "else:\n",
    "    task_understood['criteria_d']=1\n",
    "    print(\"Didn't meet criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_df=create_df(\"Fear\")\n",
    "plt.plot(fear_df.stim_selected, 'o')\n",
    "plt.plot(fear_df.correct_stim)\n",
    "\n",
    "if fear_df.iloc[-1].reversal==7.0 and fear_df.iloc[-1].correct_count==5:\n",
    "    task_understood['timed_out_f']=0\n",
    "    print(\"Passed!\")\n",
    "else:\n",
    "    task_understood['timed_out_f']=1\n",
    "    print(\"Timed out\")\n",
    "\n",
    "\n",
    "if fear_df.iloc[0].criteria ==0:\n",
    "    task_understood['criteria_f']=0\n",
    "    print(\"Met criteria\")\n",
    "else:\n",
    "    task_understood['criteria_f']=1\n",
    "    print(\"Didn't meet criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df=create_df(\"Points\")\n",
    "plt.plot(points_df.stim_selected, 'o')\n",
    "plt.plot(points_df.correct_stim)\n",
    "\n",
    "\n",
    "if points_df.iloc[-1].reversal==7.0 and points_df.iloc[-1].correct_count==5:\n",
    "    task_understood['timed_out_p']=0\n",
    "    print(\"Passed!\")\n",
    "else:\n",
    "    task_understood['timed_out_p']=1\n",
    "    print(\"Timed out\")\n",
    "\n",
    "\n",
    "if points_df.iloc[0].criteria ==0:\n",
    "    task_understood['criteria_p']=0\n",
    "    print(\"Met criteria\")\n",
    "else:\n",
    "    task_understood['criteria_p']=1\n",
    "    print(\"Didn't meet criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=task_understood['criteria_p']+task_understood['criteria_d']+task_understood['criteria_f']\n",
    "task_understood['criteria_total']=criteria\n",
    "print(criteria[0]) ##must be less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##did they take any breaks\n",
    "if len(task[task.rt/60000>10].index) ==0:\n",
    "    task_understood['long_breaks']=\"No\"\n",
    "    print(\"no\")\n",
    "else:\n",
    "    task_understood['long_breaks']=\"Yes\"\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#did they take more than 2 hours to complete the experiment\n",
    "if df.iloc[-1].time_elapsed/60000 < 120:\n",
    "    task_understood['time_elapsed']=\"No\"\n",
    "    print(\"No\")\n",
    "else:\n",
    "    task_understood['time_elapsed']=\"Yes\"\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking they learnt the task correctly\n",
    "if task_understood.attention_checks[0]>4 and task_understood.criteria_total[0]<3 and task_understood.long_breaks[0]==\"No\" and task_understood.time_elapsed[0]==\"No\":\n",
    "    task_understood['task_understood']=\"Yes\"\n",
    "    print(\"Task understood\")\n",
    "else:\n",
    "    task_understood['task_understood']=\"No\"\n",
    "    print(\"Task failed\")\n",
    "task_understood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Demography and psychiatric information</h3>\n",
    "<p>Check that this matches the info prolific gives you</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##demography - missing from this dummy data\n",
    "demography=df[df.trial_var==\"demography\"].reset_index()\n",
    "demography\n",
    "#demography_df = pd.DataFrame([demography.loc[0].response]).drop([\"title\", \"final_goodbye\", \"feedback\"], axis=\"columns\")\n",
    "#demography_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##again psychiatric diagnosis is missing from this dummy data so won't work\n",
    "ind=df.index[df.trial_var==\"psychiatric diagnosis\"]\n",
    "diagnosis=df.iloc[ind].response[ind[0]]\n",
    "diagnosis=pd.DataFrame([diagnosis])\n",
    "diagnosis.rename(columns={\"Q0\": \"diagnosis\"}, inplace=True)\n",
    "diagnosis['prolific_id']=df[0:1].prolific_id[0]\n",
    "diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Digit span task</h3>\n",
    "<p>Check that they didn't score 0</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_digit_span(df):\n",
    "    digit_span=pd.DataFrame()\n",
    "    for i in set(df.participant_no): \n",
    "        sub_df=df[df.participant_no==float(i)]\n",
    "        digit_span_df=sub_df.dropna(subset=['digit_span']).reset_index()\n",
    "        if len(digit_span_df.index)==0:\n",
    "            temp_digit_span=pd.DataFrame({'digit_span': \"task failed\", 'participant_no': [sub_df.participant_no.iloc[0]] })\n",
    "        else:\n",
    "            temp_digit_span=pd.DataFrame({'digit_span': [digit_span_df.digit_span[0]], 'participant_no': [digit_span_df.participant_no[0]]})\n",
    "        digit_span=pd.concat([digit_span, temp_digit_span])\n",
    "    return digit_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_digit_span(df) ##doesn't work in dummy data but will create df with digit span result - which should be greater than 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a19328ca44b954ed06e43a2e0ca9e2fb64feda7c5658a8f92922d9dadf68cb6c"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
